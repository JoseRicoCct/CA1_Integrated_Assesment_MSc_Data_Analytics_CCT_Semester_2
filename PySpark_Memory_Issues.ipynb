{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "007cbddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Create Database from JSON\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "\n",
    "# Read JSON file into DataFrame\n",
    "df = spark.read.format(\"json\") \\\n",
    "    .load(r\"C:\\Users\\jose\\Downloads\\goodreads_reviews_fantasy_paranormal.json\")\n",
    "\n",
    "# Create a database\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS goodreads_reviews\")\n",
    "\n",
    "# Use the database\n",
    "spark.sql(\"USE goodreads_reviews\")\n",
    "\n",
    "# Save DataFrame as a table in the database\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"reviews\")\n",
    "\n",
    "# Save DataFrame as a table in a database\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"goodreads_reviews.reviews\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f8f7cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+----------+-------+------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| book_id|          date_added|        date_updated|n_comments|n_votes|rating|             read_at|           review_id|         review_text|          started_at|             user_id|\n",
      "+--------+--------------------+--------------------+----------+-------+------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|26079201|Sun Nov 27 20:03:...|Sat Feb 04 03:09:...|         0|      0|     4|Tue Nov 29 00:00:...|ccca1430b2b25b44c...|Thank you to Raye...|Sun Nov 27 00:00:...|d043e5bbcc1135529...|\n",
      "|31207141|Tue Nov 22 03:41:...|Sat Jan 28 09:50:...|         0|      2|     4|Fri Jan 27 00:00:...|e31bd4dc601d662b4...|Review also poste...|Wed Jan 25 00:00:...|d043e5bbcc1135529...|\n",
      "|25151797|Fri Nov 04 19:45:...|Tue Nov 22 01:20:...|         0|      0|     3|Mon Nov 21 00:00:...|4583dd8f12dd00735...|Review also poste...|Fri Nov 18 00:00:...|d043e5bbcc1135529...|\n",
      "|31342605|Fri Nov 04 19:45:...|Sat Nov 26 19:23:...|         0|      0|     4|Wed Nov 23 00:00:...|556c7364116af51e2...|Review also poste...|Mon Nov 21 00:00:...|d043e5bbcc1135529...|\n",
      "|32608639|Sun Oct 30 20:45:...|Sun Oct 30 20:46:...|         1|      1|     5|Sat Oct 29 00:00:...|65005bc2b4bfc86bc...|Review also poste...|                    |d043e5bbcc1135529...|\n",
      "|28114396|Wed Oct 12 17:06:...|Sat Jan 21 11:43:...|         0|      0|     4|Fri Jan 20 00:00:...|9cc90e511ff1108af...|Review also poste...|Wed Jan 18 00:00:...|d043e5bbcc1135529...|\n",
      "|31349010|Mon Sep 19 17:13:...|Mon Sep 19 17:14:...|         0|      1|     5|Mon Sep 19 00:00:...|a6bd36edd3c876b0e...|Review also poste...|Mon Sep 19 00:00:...|d043e5bbcc1135529...|\n",
      "|26219455|Thu Sep 15 23:58:...|Sat Apr 22 03:48:...|         0|      0|     5|Wed Apr 19 00:34:...|33c6cfa7baaf44820...|Review also poste...|Tue Apr 18 17:56:...|d043e5bbcc1135529...|\n",
      "|31849588|Wed Aug 24 11:07:...|Sun Oct 30 06:10:...|         0|      0|     5|Tue Aug 23 00:00:...|21404647c030da430...|Review also poste...|                    |d043e5bbcc1135529...|\n",
      "|31849587|Fri Aug 19 10:10:...|Sun Oct 30 06:12:...|         0|      0|     5|Thu Aug 18 00:00:...|791ff07632260f9cf...|Review also poste...|                    |d043e5bbcc1135529...|\n",
      "|31835951|Tue Aug 09 06:55:...|Sun Oct 30 06:17:...|         0|      0|     4|Tue Aug 09 06:56:...|0fbda0d98079423cd...|Review also poste...|                    |d043e5bbcc1135529...|\n",
      "|23014836|Mon Aug 08 13:21:...|Wed Jul 12 03:23:...|         0|      2|     3|Wed Jul 12 03:23:...|64791cf630aaa7f12...|Review also poste...|Mon Jun 26 00:00:...|d043e5bbcc1135529...|\n",
      "|28235555|Wed Jul 20 02:30:...|Sun Oct 23 20:17:...|         0|      0|     5|Fri Jan 01 00:00:...|c793914cfdca58d99...|Review also poste...|                    |d043e5bbcc1135529...|\n",
      "|26805518|Sat Jun 11 00:06:...|Sat Jun 11 00:07:...|         0|      0|     4|Fri Jun 10 00:00:...|a67545cf06d8da1e4...|Review also poste...|                    |d043e5bbcc1135529...|\n",
      "|29546138|Wed Jun 08 20:52:...|Thu Jun 09 14:16:...|         1|      0|     3|                    |80f5c64f78bca517f...|Review also poste...|                    |d043e5bbcc1135529...|\n",
      "|26114202|Fri May 20 14:38:...|Fri Aug 05 02:36:...|         0|      0|     4|Fri Aug 05 02:36:...|169949fdef1dc0edd...|Review also poste...|                    |d043e5bbcc1135529...|\n",
      "|18584855|Tue May 17 17:53:...|Wed Feb 15 05:43:...|         0|      0|     5|Sun Feb 12 00:00:...|ff66bf556d1fa3b8f...| Review coming soon!|Fri Feb 10 00:00:...|d043e5bbcc1135529...|\n",
      "|29569157|Fri May 13 18:48:...|Fri Jul 08 20:29:...|         0|      0|     4|Thu Jul 07 00:00:...|578f91919305641ff...|Review also poste...|Thu Jul 07 00:00:...|d043e5bbcc1135529...|\n",
      "|27396942|Wed May 11 00:59:...|Tue Jul 26 21:51:...|         0|      0|     3|Wed Jul 27 21:51:...|4b0aefe36122fd9c3...|Review also poste...|                    |d043e5bbcc1135529...|\n",
      "|26114621|Sun Mar 06 10:00:...|Wed May 04 02:13:...|         0|      0|     5|                    |69b9fe740a3d4a2c5...|Review also poste...|                    |d043e5bbcc1135529...|\n",
      "+--------+--------------------+--------------------+----------+-------+------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the database\n",
    "spark.sql(\"USE goodreads_reviews\")\n",
    "\n",
    "# Query the data from the table\n",
    "result = spark.sql(\"SELECT * FROM reviews\")\n",
    "\n",
    "# Show the result\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "775514f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame as parquet files (a columnar storage format) in a directory\n",
    "df.write.mode(\"overwrite\").parquet(r\"C:\\Users\\jose\\OneDrive - Dublin Business School (DBS)\\Desktop\\CA1_Integrated_Assesment_MSc_Data_Analytics_CCT_Semester_2\\parquet_2\")\n",
    "\n",
    "# Register the DataFrame as a temporary view\n",
    "df.createOrReplaceTempView(\"reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5106885a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Create Database from Parquet\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create a database\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS goodreads_reviews\")\n",
    "\n",
    "# Select the database\n",
    "spark.sql(\"USE goodreads_reviews\")\n",
    "\n",
    "# Save DataFrame as a table in the database\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS reviews USING parquet OPTIONS (PATH 'C:/Users/jose/OneDrive - Dublin Business School (DBS)/Desktop/CA1_Integrated_Assesment_MSc_Data_Analytics_CCT_Semester_2/parquet_2')\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ca8b4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------+-----------+\n",
      "|        namespace|tableName|isTemporary|\n",
      "+-----------------+---------+-----------+\n",
      "|goodreads_reviews|  reviews|      false|\n",
      "+-----------------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show tables in the current schema\n",
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91116bf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d45ced18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+----------+-------+------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| book_id|          date_added|        date_updated|n_comments|n_votes|rating|             read_at|           review_id|         review_text|          started_at|             user_id|\n",
      "+--------+--------------------+--------------------+----------+-------+------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|26079201|Sun Nov 27 20:03:...|Sat Feb 04 03:09:...|         0|      0|     4|Tue Nov 29 00:00:...|ccca1430b2b25b44c...|Thank you to Raye...|Sun Nov 27 00:00:...|d043e5bbcc1135529...|\n",
      "|31207141|Tue Nov 22 03:41:...|Sat Jan 28 09:50:...|         0|      2|     4|Fri Jan 27 00:00:...|e31bd4dc601d662b4...|Review also poste...|Wed Jan 25 00:00:...|d043e5bbcc1135529...|\n",
      "|25151797|Fri Nov 04 19:45:...|Tue Nov 22 01:20:...|         0|      0|     3|Mon Nov 21 00:00:...|4583dd8f12dd00735...|Review also poste...|Fri Nov 18 00:00:...|d043e5bbcc1135529...|\n",
      "|31342605|Fri Nov 04 19:45:...|Sat Nov 26 19:23:...|         0|      0|     4|Wed Nov 23 00:00:...|556c7364116af51e2...|Review also poste...|Mon Nov 21 00:00:...|d043e5bbcc1135529...|\n",
      "|32608639|Sun Oct 30 20:45:...|Sun Oct 30 20:46:...|         1|      1|     5|Sat Oct 29 00:00:...|65005bc2b4bfc86bc...|Review also poste...|                    |d043e5bbcc1135529...|\n",
      "|28114396|Wed Oct 12 17:06:...|Sat Jan 21 11:43:...|         0|      0|     4|Fri Jan 20 00:00:...|9cc90e511ff1108af...|Review also poste...|Wed Jan 18 00:00:...|d043e5bbcc1135529...|\n",
      "|31349010|Mon Sep 19 17:13:...|Mon Sep 19 17:14:...|         0|      1|     5|Mon Sep 19 00:00:...|a6bd36edd3c876b0e...|Review also poste...|Mon Sep 19 00:00:...|d043e5bbcc1135529...|\n",
      "|26219455|Thu Sep 15 23:58:...|Sat Apr 22 03:48:...|         0|      0|     5|Wed Apr 19 00:34:...|33c6cfa7baaf44820...|Review also poste...|Tue Apr 18 17:56:...|d043e5bbcc1135529...|\n",
      "|31849588|Wed Aug 24 11:07:...|Sun Oct 30 06:10:...|         0|      0|     5|Tue Aug 23 00:00:...|21404647c030da430...|Review also poste...|                    |d043e5bbcc1135529...|\n",
      "|31849587|Fri Aug 19 10:10:...|Sun Oct 30 06:12:...|         0|      0|     5|Thu Aug 18 00:00:...|791ff07632260f9cf...|Review also poste...|                    |d043e5bbcc1135529...|\n",
      "|31835951|Tue Aug 09 06:55:...|Sun Oct 30 06:17:...|         0|      0|     4|Tue Aug 09 06:56:...|0fbda0d98079423cd...|Review also poste...|                    |d043e5bbcc1135529...|\n",
      "|23014836|Mon Aug 08 13:21:...|Wed Jul 12 03:23:...|         0|      2|     3|Wed Jul 12 03:23:...|64791cf630aaa7f12...|Review also poste...|Mon Jun 26 00:00:...|d043e5bbcc1135529...|\n",
      "|28235555|Wed Jul 20 02:30:...|Sun Oct 23 20:17:...|         0|      0|     5|Fri Jan 01 00:00:...|c793914cfdca58d99...|Review also poste...|                    |d043e5bbcc1135529...|\n",
      "|26805518|Sat Jun 11 00:06:...|Sat Jun 11 00:07:...|         0|      0|     4|Fri Jun 10 00:00:...|a67545cf06d8da1e4...|Review also poste...|                    |d043e5bbcc1135529...|\n",
      "|29546138|Wed Jun 08 20:52:...|Thu Jun 09 14:16:...|         1|      0|     3|                    |80f5c64f78bca517f...|Review also poste...|                    |d043e5bbcc1135529...|\n",
      "|26114202|Fri May 20 14:38:...|Fri Aug 05 02:36:...|         0|      0|     4|Fri Aug 05 02:36:...|169949fdef1dc0edd...|Review also poste...|                    |d043e5bbcc1135529...|\n",
      "|18584855|Tue May 17 17:53:...|Wed Feb 15 05:43:...|         0|      0|     5|Sun Feb 12 00:00:...|ff66bf556d1fa3b8f...| Review coming soon!|Fri Feb 10 00:00:...|d043e5bbcc1135529...|\n",
      "|29569157|Fri May 13 18:48:...|Fri Jul 08 20:29:...|         0|      0|     4|Thu Jul 07 00:00:...|578f91919305641ff...|Review also poste...|Thu Jul 07 00:00:...|d043e5bbcc1135529...|\n",
      "|27396942|Wed May 11 00:59:...|Tue Jul 26 21:51:...|         0|      0|     3|Wed Jul 27 21:51:...|4b0aefe36122fd9c3...|Review also poste...|                    |d043e5bbcc1135529...|\n",
      "|26114621|Sun Mar 06 10:00:...|Wed May 04 02:13:...|         0|      0|     5|                    |69b9fe740a3d4a2c5...|Review also poste...|                    |d043e5bbcc1135529...|\n",
      "+--------+--------------------+--------------------+----------+-------+------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the database\n",
    "spark.sql(\"USE goodreads_reviews\")\n",
    "\n",
    "# Query the data from the table\n",
    "result = spark.sql(\"SELECT * FROM reviews\")\n",
    "\n",
    "# Show the result\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97c40aec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_id</th>\n",
       "      <th>date_added</th>\n",
       "      <th>date_updated</th>\n",
       "      <th>n_comments</th>\n",
       "      <th>n_votes</th>\n",
       "      <th>rating</th>\n",
       "      <th>read_at</th>\n",
       "      <th>review_id</th>\n",
       "      <th>review_text</th>\n",
       "      <th>started_at</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26079201</td>\n",
       "      <td>Sun Nov 27 20:03:02 -0800 2016</td>\n",
       "      <td>Sat Feb 04 03:09:37 -0800 2017</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>Tue Nov 29 00:00:00 -0800 2016</td>\n",
       "      <td>ccca1430b2b25b44ce1baaf0d13849ee</td>\n",
       "      <td>Thank you to Raye Wagner for providing me with...</td>\n",
       "      <td>Sun Nov 27 00:00:00 -0800 2016</td>\n",
       "      <td>d043e5bbcc1135529327faf7260fecaa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31207141</td>\n",
       "      <td>Tue Nov 22 03:41:56 -0800 2016</td>\n",
       "      <td>Sat Jan 28 09:50:35 -0800 2017</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>Fri Jan 27 00:00:00 -0800 2017</td>\n",
       "      <td>e31bd4dc601d662b405ac00bcf834160</td>\n",
       "      <td>Review also posted at: http://underthebookcove...</td>\n",
       "      <td>Wed Jan 25 00:00:00 -0800 2017</td>\n",
       "      <td>d043e5bbcc1135529327faf7260fecaa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25151797</td>\n",
       "      <td>Fri Nov 04 19:45:39 -0700 2016</td>\n",
       "      <td>Tue Nov 22 01:20:27 -0800 2016</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Mon Nov 21 00:00:00 -0800 2016</td>\n",
       "      <td>4583dd8f12dd00735f558d98019b95c8</td>\n",
       "      <td>Review also posted at: http://underthebookcove...</td>\n",
       "      <td>Fri Nov 18 00:00:00 -0800 2016</td>\n",
       "      <td>d043e5bbcc1135529327faf7260fecaa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31342605</td>\n",
       "      <td>Fri Nov 04 19:45:29 -0700 2016</td>\n",
       "      <td>Sat Nov 26 19:23:06 -0800 2016</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>Wed Nov 23 00:00:00 -0800 2016</td>\n",
       "      <td>556c7364116af51e25cecd39ea7998be</td>\n",
       "      <td>Review also posted at: http://underthebookcove...</td>\n",
       "      <td>Mon Nov 21 00:00:00 -0800 2016</td>\n",
       "      <td>d043e5bbcc1135529327faf7260fecaa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32608639</td>\n",
       "      <td>Sun Oct 30 20:45:17 -0700 2016</td>\n",
       "      <td>Sun Oct 30 20:46:31 -0700 2016</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Sat Oct 29 00:00:00 -0700 2016</td>\n",
       "      <td>65005bc2b4bfc86bcb8a72e26099e790</td>\n",
       "      <td>Review also posted at: http://underthebookcove...</td>\n",
       "      <td></td>\n",
       "      <td>d043e5bbcc1135529327faf7260fecaa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>28114396</td>\n",
       "      <td>Wed Oct 12 17:06:19 -0700 2016</td>\n",
       "      <td>Sat Jan 21 11:43:12 -0800 2017</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>Fri Jan 20 00:00:00 -0800 2017</td>\n",
       "      <td>9cc90e511ff1108afc908bf33f258f31</td>\n",
       "      <td>Review also posted at: http://underthebookcove...</td>\n",
       "      <td>Wed Jan 18 00:00:00 -0800 2017</td>\n",
       "      <td>d043e5bbcc1135529327faf7260fecaa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>31349010</td>\n",
       "      <td>Mon Sep 19 17:13:17 -0700 2016</td>\n",
       "      <td>Mon Sep 19 17:14:41 -0700 2016</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Mon Sep 19 00:00:00 -0700 2016</td>\n",
       "      <td>a6bd36edd3c876b0edae0a5d250e4b67</td>\n",
       "      <td>Review also posted at: http://underthebookcove...</td>\n",
       "      <td>Mon Sep 19 00:00:00 -0700 2016</td>\n",
       "      <td>d043e5bbcc1135529327faf7260fecaa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>26219455</td>\n",
       "      <td>Thu Sep 15 23:58:25 -0700 2016</td>\n",
       "      <td>Sat Apr 22 03:48:38 -0700 2017</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>Wed Apr 19 00:34:53 -0700 2017</td>\n",
       "      <td>33c6cfa7baaf44820765ed431e74d5b3</td>\n",
       "      <td>Review also posted at: http://underthebookcove...</td>\n",
       "      <td>Tue Apr 18 17:56:12 -0700 2017</td>\n",
       "      <td>d043e5bbcc1135529327faf7260fecaa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>31849588</td>\n",
       "      <td>Wed Aug 24 11:07:20 -0700 2016</td>\n",
       "      <td>Sun Oct 30 06:10:16 -0700 2016</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>Tue Aug 23 00:00:00 -0700 2016</td>\n",
       "      <td>21404647c030da430b617cc91d98a7e7</td>\n",
       "      <td>Review also posted at: http://underthebookcove...</td>\n",
       "      <td></td>\n",
       "      <td>d043e5bbcc1135529327faf7260fecaa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>31849587</td>\n",
       "      <td>Fri Aug 19 10:10:54 -0700 2016</td>\n",
       "      <td>Sun Oct 30 06:12:17 -0700 2016</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>Thu Aug 18 00:00:00 -0700 2016</td>\n",
       "      <td>791ff07632260f9cfb162e24500b4561</td>\n",
       "      <td>Review also posted at: http://underthebookcove...</td>\n",
       "      <td></td>\n",
       "      <td>d043e5bbcc1135529327faf7260fecaa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    book_id                      date_added                    date_updated  \\\n",
       "0  26079201  Sun Nov 27 20:03:02 -0800 2016  Sat Feb 04 03:09:37 -0800 2017   \n",
       "1  31207141  Tue Nov 22 03:41:56 -0800 2016  Sat Jan 28 09:50:35 -0800 2017   \n",
       "2  25151797  Fri Nov 04 19:45:39 -0700 2016  Tue Nov 22 01:20:27 -0800 2016   \n",
       "3  31342605  Fri Nov 04 19:45:29 -0700 2016  Sat Nov 26 19:23:06 -0800 2016   \n",
       "4  32608639  Sun Oct 30 20:45:17 -0700 2016  Sun Oct 30 20:46:31 -0700 2016   \n",
       "5  28114396  Wed Oct 12 17:06:19 -0700 2016  Sat Jan 21 11:43:12 -0800 2017   \n",
       "6  31349010  Mon Sep 19 17:13:17 -0700 2016  Mon Sep 19 17:14:41 -0700 2016   \n",
       "7  26219455  Thu Sep 15 23:58:25 -0700 2016  Sat Apr 22 03:48:38 -0700 2017   \n",
       "8  31849588  Wed Aug 24 11:07:20 -0700 2016  Sun Oct 30 06:10:16 -0700 2016   \n",
       "9  31849587  Fri Aug 19 10:10:54 -0700 2016  Sun Oct 30 06:12:17 -0700 2016   \n",
       "\n",
       "   n_comments  n_votes  rating                         read_at  \\\n",
       "0           0        0       4  Tue Nov 29 00:00:00 -0800 2016   \n",
       "1           0        2       4  Fri Jan 27 00:00:00 -0800 2017   \n",
       "2           0        0       3  Mon Nov 21 00:00:00 -0800 2016   \n",
       "3           0        0       4  Wed Nov 23 00:00:00 -0800 2016   \n",
       "4           1        1       5  Sat Oct 29 00:00:00 -0700 2016   \n",
       "5           0        0       4  Fri Jan 20 00:00:00 -0800 2017   \n",
       "6           0        1       5  Mon Sep 19 00:00:00 -0700 2016   \n",
       "7           0        0       5  Wed Apr 19 00:34:53 -0700 2017   \n",
       "8           0        0       5  Tue Aug 23 00:00:00 -0700 2016   \n",
       "9           0        0       5  Thu Aug 18 00:00:00 -0700 2016   \n",
       "\n",
       "                          review_id  \\\n",
       "0  ccca1430b2b25b44ce1baaf0d13849ee   \n",
       "1  e31bd4dc601d662b405ac00bcf834160   \n",
       "2  4583dd8f12dd00735f558d98019b95c8   \n",
       "3  556c7364116af51e25cecd39ea7998be   \n",
       "4  65005bc2b4bfc86bcb8a72e26099e790   \n",
       "5  9cc90e511ff1108afc908bf33f258f31   \n",
       "6  a6bd36edd3c876b0edae0a5d250e4b67   \n",
       "7  33c6cfa7baaf44820765ed431e74d5b3   \n",
       "8  21404647c030da430b617cc91d98a7e7   \n",
       "9  791ff07632260f9cfb162e24500b4561   \n",
       "\n",
       "                                         review_text  \\\n",
       "0  Thank you to Raye Wagner for providing me with...   \n",
       "1  Review also posted at: http://underthebookcove...   \n",
       "2  Review also posted at: http://underthebookcove...   \n",
       "3  Review also posted at: http://underthebookcove...   \n",
       "4  Review also posted at: http://underthebookcove...   \n",
       "5  Review also posted at: http://underthebookcove...   \n",
       "6  Review also posted at: http://underthebookcove...   \n",
       "7  Review also posted at: http://underthebookcove...   \n",
       "8  Review also posted at: http://underthebookcove...   \n",
       "9  Review also posted at: http://underthebookcove...   \n",
       "\n",
       "                       started_at                           user_id  \n",
       "0  Sun Nov 27 00:00:00 -0800 2016  d043e5bbcc1135529327faf7260fecaa  \n",
       "1  Wed Jan 25 00:00:00 -0800 2017  d043e5bbcc1135529327faf7260fecaa  \n",
       "2  Fri Nov 18 00:00:00 -0800 2016  d043e5bbcc1135529327faf7260fecaa  \n",
       "3  Mon Nov 21 00:00:00 -0800 2016  d043e5bbcc1135529327faf7260fecaa  \n",
       "4                                  d043e5bbcc1135529327faf7260fecaa  \n",
       "5  Wed Jan 18 00:00:00 -0800 2017  d043e5bbcc1135529327faf7260fecaa  \n",
       "6  Mon Sep 19 00:00:00 -0700 2016  d043e5bbcc1135529327faf7260fecaa  \n",
       "7  Tue Apr 18 17:56:12 -0700 2017  d043e5bbcc1135529327faf7260fecaa  \n",
       "8                                  d043e5bbcc1135529327faf7260fecaa  \n",
       "9                                  d043e5bbcc1135529327faf7260fecaa  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Limiting the number of columns to display\n",
    "result.limit(10).toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5de77c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+--------------------+--------------------+\n",
      "| book_id|rating|         review_text|             user_id|\n",
      "+--------+------+--------------------+--------------------+\n",
      "|26079201|     4|Thank you to Raye...|d043e5bbcc1135529...|\n",
      "|31207141|     4|Review also poste...|d043e5bbcc1135529...|\n",
      "|25151797|     3|Review also poste...|d043e5bbcc1135529...|\n",
      "|31342605|     4|Review also poste...|d043e5bbcc1135529...|\n",
      "|32608639|     5|Review also poste...|d043e5bbcc1135529...|\n",
      "|28114396|     4|Review also poste...|d043e5bbcc1135529...|\n",
      "|31349010|     5|Review also poste...|d043e5bbcc1135529...|\n",
      "|26219455|     5|Review also poste...|d043e5bbcc1135529...|\n",
      "|31849588|     5|Review also poste...|d043e5bbcc1135529...|\n",
      "|31849587|     5|Review also poste...|d043e5bbcc1135529...|\n",
      "|31835951|     4|Review also poste...|d043e5bbcc1135529...|\n",
      "|23014836|     3|Review also poste...|d043e5bbcc1135529...|\n",
      "|28235555|     5|Review also poste...|d043e5bbcc1135529...|\n",
      "|26805518|     4|Review also poste...|d043e5bbcc1135529...|\n",
      "|29546138|     3|Review also poste...|d043e5bbcc1135529...|\n",
      "|26114202|     4|Review also poste...|d043e5bbcc1135529...|\n",
      "|18584855|     5| Review coming soon!|d043e5bbcc1135529...|\n",
      "|29569157|     4|Review also poste...|d043e5bbcc1135529...|\n",
      "|27396942|     3|Review also poste...|d043e5bbcc1135529...|\n",
      "|26114621|     5|Review also poste...|d043e5bbcc1135529...|\n",
      "+--------+------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming df is your Spark DataFrame\n",
    "columns_to_drop = [\"date_added\", \"date_updated\",\"n_comments\",\"n_votes\",\"review_id\",\"started_at\",\"read_at\"]  # List of columns to drop\n",
    "\n",
    "# Drop the specified columns\n",
    "result = result.drop(*columns_to_drop)\n",
    "\n",
    "# Show the result\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04529195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----------+-------+\n",
      "|book_id| rating|review_text|user_id|\n",
      "+-------+-------+-----------+-------+\n",
      "|3424641|3424641|    3424641|3424641|\n",
      "+-------+-------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count\n",
    "\n",
    "# Cast the review_text column to string type\n",
    "result = result.withColumn(\"review_text\", col(\"review_text\").cast(\"string\"))\n",
    "\n",
    "# Assuming df is your Spark DataFrame\n",
    "# Select each column and count the number of null values in each column\n",
    "null_counts = result.select(*(count(col(c)).alias(c) for c in result.columns))\n",
    "\n",
    "# Show the result\n",
    "null_counts.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5166cfd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+--------------------+--------------------+\n",
      "| book_id|rating|         review_text|             user_id|\n",
      "+--------+------+--------------------+--------------------+\n",
      "|26079201|     4|Thank you to Raye...|d043e5bbcc1135529...|\n",
      "|31207141|     4|Review also poste...|d043e5bbcc1135529...|\n",
      "|25151797|     3|Review also poste...|d043e5bbcc1135529...|\n",
      "|31342605|     4|Review also poste...|d043e5bbcc1135529...|\n",
      "|32608639|     5|Review also poste...|d043e5bbcc1135529...|\n",
      "|28114396|     4|Review also poste...|d043e5bbcc1135529...|\n",
      "|31349010|     5|Review also poste...|d043e5bbcc1135529...|\n",
      "|26219455|     5|Review also poste...|d043e5bbcc1135529...|\n",
      "|31849588|     5|Review also poste...|d043e5bbcc1135529...|\n",
      "|31849587|     5|Review also poste...|d043e5bbcc1135529...|\n",
      "|31835951|     4|Review also poste...|d043e5bbcc1135529...|\n",
      "|23014836|     3|Review also poste...|d043e5bbcc1135529...|\n",
      "|28235555|     5|Review also poste...|d043e5bbcc1135529...|\n",
      "|26805518|     4|Review also poste...|d043e5bbcc1135529...|\n",
      "|29546138|     3|Review also poste...|d043e5bbcc1135529...|\n",
      "|26114202|     4|Review also poste...|d043e5bbcc1135529...|\n",
      "|18584855|     5| Review coming soon!|d043e5bbcc1135529...|\n",
      "|29569157|     4|Review also poste...|d043e5bbcc1135529...|\n",
      "|27396942|     3|Review also poste...|d043e5bbcc1135529...|\n",
      "|26114621|     5|Review also poste...|d043e5bbcc1135529...|\n",
      "+--------+------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming df is your Spark DataFrame\n",
    "# Drop rows with any null values\n",
    "result = result.dropna()\n",
    "\n",
    "# Show the cleaned DataFrame\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60341105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+--------------------+\n",
      "| book_id|rating|             user_id|\n",
      "+--------+------+--------------------+\n",
      "|26079201|     4|d043e5bbcc1135529...|\n",
      "|31207141|     4|d043e5bbcc1135529...|\n",
      "|25151797|     3|d043e5bbcc1135529...|\n",
      "|31342605|     4|d043e5bbcc1135529...|\n",
      "|32608639|     5|d043e5bbcc1135529...|\n",
      "|28114396|     4|d043e5bbcc1135529...|\n",
      "|31349010|     5|d043e5bbcc1135529...|\n",
      "|26219455|     5|d043e5bbcc1135529...|\n",
      "|31849588|     5|d043e5bbcc1135529...|\n",
      "|31849587|     5|d043e5bbcc1135529...|\n",
      "|31835951|     4|d043e5bbcc1135529...|\n",
      "|23014836|     3|d043e5bbcc1135529...|\n",
      "|28235555|     5|d043e5bbcc1135529...|\n",
      "|26805518|     4|d043e5bbcc1135529...|\n",
      "|29546138|     3|d043e5bbcc1135529...|\n",
      "|26114202|     4|d043e5bbcc1135529...|\n",
      "|18584855|     5|d043e5bbcc1135529...|\n",
      "|29569157|     4|d043e5bbcc1135529...|\n",
      "|27396942|     3|d043e5bbcc1135529...|\n",
      "|26114621|     5|d043e5bbcc1135529...|\n",
      "+--------+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming df is your Spark DataFrame\n",
    "columns_to_drop = [\"review_text\"]  # List of columns to drop\n",
    "\n",
    "# Drop the specified columns\n",
    "result = result.drop(*columns_to_drop)\n",
    "\n",
    "# Show the result\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7140dc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *  \n",
    "from pyspark.sql.functions import *  \n",
    "from pyspark.sql.types import *  \n",
    "import numpy as np    \n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "reviews_pandas_df= result.select(\"*\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d8fe11e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26079201</td>\n",
       "      <td>4</td>\n",
       "      <td>d043e5bbcc1135529327faf7260fecaa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31207141</td>\n",
       "      <td>4</td>\n",
       "      <td>d043e5bbcc1135529327faf7260fecaa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25151797</td>\n",
       "      <td>3</td>\n",
       "      <td>d043e5bbcc1135529327faf7260fecaa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31342605</td>\n",
       "      <td>4</td>\n",
       "      <td>d043e5bbcc1135529327faf7260fecaa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32608639</td>\n",
       "      <td>5</td>\n",
       "      <td>d043e5bbcc1135529327faf7260fecaa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3424636</th>\n",
       "      <td>5907</td>\n",
       "      <td>5</td>\n",
       "      <td>9a1e8a72fadaa8f3038aa783114e12cc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3424637</th>\n",
       "      <td>6137154</td>\n",
       "      <td>5</td>\n",
       "      <td>a13e16a58266ffa4a0851c417c4cda8f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3424638</th>\n",
       "      <td>41804</td>\n",
       "      <td>4</td>\n",
       "      <td>007c41115862813421ff17dde43728b3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3424639</th>\n",
       "      <td>14740456</td>\n",
       "      <td>4</td>\n",
       "      <td>fc5aea6994fc754daefa27cbea10ac79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3424640</th>\n",
       "      <td>1656001</td>\n",
       "      <td>4</td>\n",
       "      <td>d0f6d1a4edcab80a6010cfcfeda4999f</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3424641 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          book_id  rating                           user_id\n",
       "0        26079201       4  d043e5bbcc1135529327faf7260fecaa\n",
       "1        31207141       4  d043e5bbcc1135529327faf7260fecaa\n",
       "2        25151797       3  d043e5bbcc1135529327faf7260fecaa\n",
       "3        31342605       4  d043e5bbcc1135529327faf7260fecaa\n",
       "4        32608639       5  d043e5bbcc1135529327faf7260fecaa\n",
       "...           ...     ...                               ...\n",
       "3424636      5907       5  9a1e8a72fadaa8f3038aa783114e12cc\n",
       "3424637   6137154       5  a13e16a58266ffa4a0851c417c4cda8f\n",
       "3424638     41804       4  007c41115862813421ff17dde43728b3\n",
       "3424639  14740456       4  fc5aea6994fc754daefa27cbea10ac79\n",
       "3424640   1656001       4  d0f6d1a4edcab80a6010cfcfeda4999f\n",
       "\n",
       "[3424641 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_pandas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a17ea380",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o49.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Total size of serialized results of 11 tasks (1145.7 MiB) is bigger than spark.driver.maxResultSize (1024.0 MiB)\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4148)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\r\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4145)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Convert Spark DataFrame to Pandas DataFrame in chunks\u001b[39;00m\n\u001b[0;32m      7\u001b[0m reviews_pandas_chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m result\u001b[38;5;241m.\u001b[39mtoPandas()\u001b[38;5;241m.\u001b[39mto_numpy()\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, chunk_size):\n\u001b[0;32m      9\u001b[0m     pandas_chunk \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(chunk, columns\u001b[38;5;241m=\u001b[39mresult\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m     10\u001b[0m     reviews_pandas_chunks\u001b[38;5;241m.\u001b[39mappend(pandas_chunk)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    199\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;66;03m# Below is toPandas without Arrow optimization.\u001b[39;00m\n\u001b[1;32m--> 202\u001b[0m rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(rows) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    204\u001b[0m     pdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_records(\n\u001b[0;32m    205\u001b[0m         rows, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(rows)), columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    206\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:1261\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1241\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[0;32m   1242\u001b[0m \n\u001b[0;32m   1243\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1258\u001b[0m \u001b[38;5;124;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[0;32m   1259\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1260\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[1;32m-> 1261\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mcollectToPython()\n\u001b[0;32m   1262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o49.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Total size of serialized results of 11 tasks (1145.7 MiB) is bigger than spark.driver.maxResultSize (1024.0 MiB)\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4148)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\r\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4145)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the number of rows per chunk\n",
    "chunk_size = 1000  # Adjust the chunk size as needed\n",
    "\n",
    "# Convert Spark DataFrame to Pandas DataFrame in chunks\n",
    "reviews_pandas_chunks = []\n",
    "for chunk in result.toPandas().to_numpy().reshape(-1, chunk_size):\n",
    "    pandas_chunk = pd.DataFrame(chunk, columns=result.columns)\n",
    "    reviews_pandas_chunks.append(pandas_chunk)\n",
    "\n",
    "# Concatenate chunks into a single Pandas DataFrame\n",
    "reviews_pandas_df = pd.concat(reviews_pandas_chunks, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4809d05e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reviews_pandas_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m reviews_pandas_df\n",
      "\u001b[1;31mNameError\u001b[0m: name 'reviews_pandas_df' is not defined"
     ]
    }
   ],
   "source": [
    "reviews_pandas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "879d98ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o56.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Total size of serialized results of 11 tasks (1145.7 MiB) is bigger than spark.driver.maxResultSize (1024.0 MiB)\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4148)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\r\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4145)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m    \n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m reviews_pandas_df\u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtoPandas()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    199\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;66;03m# Below is toPandas without Arrow optimization.\u001b[39;00m\n\u001b[1;32m--> 202\u001b[0m rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(rows) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    204\u001b[0m     pdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_records(\n\u001b[0;32m    205\u001b[0m         rows, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(rows)), columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    206\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:1261\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1241\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[0;32m   1242\u001b[0m \n\u001b[0;32m   1243\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1258\u001b[0m \u001b[38;5;124;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[0;32m   1259\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1260\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[1;32m-> 1261\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mcollectToPython()\n\u001b[0;32m   1262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o56.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Total size of serialized results of 11 tasks (1145.7 MiB) is bigger than spark.driver.maxResultSize (1024.0 MiB)\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4148)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\r\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4145)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import *  \n",
    "from pyspark.sql.functions import *  \n",
    "from pyspark.sql.types import *  \n",
    "import numpy as np    \n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "reviews_pandas_df= result.select(\"*\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df43fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame as parquet files (a columnar storage format) in a directory\n",
    "df.write.mode(\"overwrite\").parquet(r\"C:\\Users\\jose\\OneDrive - Dublin Business School (DBS)\\Desktop\\CA1_Integrated_Assesment_MSc_Data_Analytics_CCT_Semester_2\\parquet_2\")\n",
    "\n",
    "# Register the DataFrame as a temporary view\n",
    "df.createOrReplaceTempView(\"reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672b350c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Create Database from CSV\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create a database\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS amz_reviews\")\n",
    "\n",
    "# Save DataFrame as a table in the database\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS reviews USING parquet OPTIONS (PATH 'C:/Users/jose/OneDrive - Dublin Business School (DBS)/Desktop/CA1_Integrated_Assesment_MSc_Data_Analytics_CCT_Semester_2/parquet_2')\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1d4e83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffcd0fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
